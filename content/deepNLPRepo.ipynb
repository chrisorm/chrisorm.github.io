{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow for Text Classification\n",
    "\n",
    "Earlier this year, I gave a talk at London Tensorflow Meetup, giving an interactive tutorial on how to do text classification using Tensorflow. This is just a short post going over the resources I created as part of that talk. The resources can be found [on my github.](https://github.com/chrisorm/LTM-July-2017-Talk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "For those wanting to learn to apply neural networks to text classification, finding practical resources can be somewhat challenging. The excellent, [CS224n](http://web.stanford.edu/class/cs224n/syllabus.html) from Stanford has some excellent theoretical resources. However, if you don't go to stanford, access to materials to do this in practice can be somewhat lacking!\n",
    "\n",
    "All the exercises below have a semi-complete set of notebooks for you to work on, and have full solutions too, in case you want something to compare against or something is unclear. I suggest you invest the time to get a minimal working example before you look at my approach - it will pay dividends!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Suggested Syllabus\n",
    "\n",
    "### Lesson 1 \n",
    "* Lecture 8 of CS224n [Slides](http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture8.pdf)\n",
    "* First part of WildML blog on RNNs [Blog](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)\n",
    "* WildML blog of RNNs in Tensorflow. [Blog](http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/) - Some features/locations may have changed between tensorflow versions.\n",
    "* Read the documentation about dynamic_rnn, and cell types, on tensorflow website. [dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn) [BasicRNNCell](http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture9.pdf)\n",
    "\n",
    "#### Exercises\n",
    "* **BasicRNN**\n",
    "\n",
    "\n",
    "### Lesson 2\n",
    "* Lecture 9 CS224n [Slides](http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-lecture9.pdf)\n",
    "* Colah's Blog on LSTMs [Blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "* Read the documentation about GRUs and LSTMs, on tensorflow website. [LSTM](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell) [GRU](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/GRUCell)\n",
    "\n",
    "#### Exercises\n",
    "* **GRURNN**\n",
    "* **LSTMRNN**\n",
    "\n",
    "### Lesson 3\n",
    "* Later part of WildML blog on RNNs [Blog](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)\n",
    "* Oxford Deep NLP course [Lecture 5](https://github.com/oxford-cs-deepnlp-2017/lectures/blob/master/Lecture%205%20-%20Text%20Classification.pdf)\n",
    "* Read tensorflow documentation about [bidirectional_dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/bidirectional_dynamic_rnn)\n",
    "#### Exercises\n",
    "* **BasicBidirectionalRNN** \n",
    "* Modify this code to run bidirectional LSTM and GRU networks.\n",
    "\n",
    "### Lesson \n",
    "* Oxford Deep NLP Conditional Language Modelling with attention [Slides](https://github.com/oxford-cs-deepnlp-2017/lectures/blob/master/Lecture%208%20-%20Conditional%20Language%20Modeling%20with%20Attention.pdf)\n",
    "\n",
    "#### Exercises\n",
    "* **BasicBidirectionalRNN-MeanPooling**\n",
    "* **BasicBidirectionalRNN-MaxPooling** \n",
    "* **GRUBidirectionalRNN-MeanPooling**\n",
    "* **GRUBidirectionalRNN-MaxPooling**\n",
    "\n",
    "### Lesson 5\n",
    "* Oxford Deep NLP Conditional Language Modelling with attention [Slides](https://github.com/oxford-cs-deepnlp-2017/lectures/blob/master/Lecture%208%20-%20Conditional%20Language%20Modeling%20with%20Attention.pdf)\n",
    "* Wild ML Post on attention [Blog](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)\n",
    "* Hierchical Attention Networks - Zhang, 2015 [Paper](https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf)\n",
    "#### Exercises\n",
    "* **BasicRNNAttention**\n",
    "* Modify this code to run attention over an LSTM network.\n",
    "\n",
    "### Lesson 6+\n",
    "Apply these techniques to other datasets, some examples:\n",
    "#### Exercises\n",
    "\n",
    "* Yelp dataset [Site](https://www.yelp.co.uk/dataset_challenge)\n",
    "* Hierichal text classification [Kaggle](https://www.kaggle.com/c/lshtc)\n",
    "* Reuters Dataset [Site](http://www.daviddlewis.com/resources/testcollections/reuters21578/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This is only a short set of notebooks, and of course their are many other approachs and techniques that one could use. However, if you invest the time and work through these, you will be incredibly well placed to tackle current research papers in the area!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
