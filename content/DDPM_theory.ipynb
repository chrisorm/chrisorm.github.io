{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b68112a6-bab2-4dd0-beaa-571858d09b00",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Models - Theory Notes\n",
    "\n",
    "## Intro\n",
    "\n",
    "This is not intended to be an exhaustive overview of DDPM theory - the following resources are helpful to understanding more;<br/>\n",
    "\n",
    "SDE theory: https://arxiv.org/pdf/2011.13456.pdf <br/>\n",
    "DDPM paper: https://arxiv.org/pdf/2006.11239.pdf<br/>\n",
    "score matching theory: https://random-walks.org/content/misc/score-matching/score-matching.html<br/>\n",
    "\n",
    "## Agenda\n",
    "\n",
    "1. Cover the intuition of Denoising Diffusion models.\n",
    "2. Diffusion Models as ELBO maximisation.\n",
    "3. Applying Reverse RPP to achieve the simplified loss function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0affcd69-dd43-41a1-9693-3ece29245c56",
   "metadata": {},
   "source": [
    "## The intuition\n",
    "\n",
    "We want to build a generative model for our data. So, we devise the following recipe;\n",
    "* Take each data point, and add a small amount of gausian noise.\n",
    "* Repeat this step adding more and more noise up to some fixed number of steps, T. \n",
    "* We should pick T and the size of the noise so that by the time we have applied T steps of noise, our original data is indistinguisable from Gaussian noise. \n",
    "* Now, we learn a neural network that learns to reverse this process;\n",
    "* To train it, we pick a t between 1 and T, and try to predict $x_{t-1}$ using $x_{t}$ and which step we are on, t.\n",
    "* However, its even easier; $x_{t} = x_{t-1} + \\epsilon$, so all we need to do is predict the noise.\n",
    "* Now sampling is clear! We start with some random noise, and gradually use our model to remove noise a bit at a time, until we get something that isn't noise.\n",
    "* If all this works well, our neural network is a (recursively applied) mapping from parts of the Gaussian domain onto structured spaces (for example, images), and so we can turn noise into unseen samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61374683-4203-43d4-8f3c-f1f3ba537525",
   "metadata": {},
   "source": [
    "## ELBO Maximisation\n",
    "\n",
    "We have a model, which gives us the probability of our data, and has parameters $\\theta$; $P_{\\theta}(X)$.<br/>\n",
    "\n",
    "However, the construction of the model is not amenable to computing P directly;\n",
    "$log P_{\\theta}(X_{0}) = log \\int_{X_1, X_2, ...,X_T}P_{\\theta}(X_{0}, X_{1}, ..., X_{T}) dX_1 dX_2 ...dX_T$\n",
    "\n",
    "This is the usual proble of variational inference, and so applying the ELBO gets us out of the mess.\n",
    "\n",
    "What is different here is the direction - we are fixing the posterior and learning a model.\n",
    "\n",
    "\n",
    "$ log \\int_{X_1, X_2, ...,X_T}P_{\\theta}(X_{0}, X_{1}, ..., X_{T}) dX_1 dX_2 ...dX_T  = log \\int_{X_1, X_2, ...,X_T}P_{\\theta}(X_{0}, X_{1}, ..., X_{T})\\frac{q(X_{1},...X_{T})}{q(X_{1},...X_{T}|X_{0})} dX_1 dX_2 ...dX_T$<br/><br/>\n",
    "$ = log E_{q}[\\frac{P_{\\theta}(X_{0}, X_{1}, ..., X_{T})}{q(X_{1},...X_{T}|X_{0})}]$<br/><br/>\n",
    "$ \\geq E_{q}[log \\frac{P_{\\theta}(X_{0}, X_{1}, ..., X_{T})}{q(X_{1},...X_{T}|X_{0})}] = \\mathcal{L}$<br/><br/>\n",
    "\n",
    "And we have the markov decomposition $q(X_{1},...X_{T}| X_{0}) = \\prod_{t=1}^{T} q(X_{t}| X_{t-1})$ and $P_{\\theta}(X_{0}, X_{1}, ..., X_{T}) = P(X_{T})\\prod_{t=1}^{T}P_{\\theta}(X_{t-1}|X_{t})$\n",
    "\n",
    "\n",
    "Applying these we get; <br/><br/>\n",
    "\n",
    "$\\mathcal{L} = E_{q}[-log P_{\\theta}(X_{T}) - \\sum_{t \\geq 1} \\frac{log P_{\\theta}(X_{t-1}|X_{t})}{q(X_{t}|X_{t-1})}]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5990325-269a-4094-ae7f-b1d5677dab1c",
   "metadata": {},
   "source": [
    "## Reverse process conditioning\n",
    "\n",
    "Next, a small rewrite of this objective, to express the loss as a sum of KL divergences, helps us because we have very efficient tools for computing the KL divergence between two gaussians.\n",
    "\n",
    "\n",
    "$E_{q}[-log P_{\\theta}(X_{T}) - \\sum_{t \\gt 1} \\frac{log P_{\\theta}(X_{t-1}|X_{t})}{q(X_{t}|X_{t-1})} - log \\frac{P_{\\theta}(X_{0}|X_{1})}{q(X_{1}|X_{0})}]$\n",
    "\n",
    "Bayes (and conditional indepedence) gives us;\n",
    "\n",
    "$q(X_{t}|X_{t-1}) = q(X_{t}|X_{t-1}, X_{0}) = \\frac{q(X_{t-1}|X_{t}, X_{0}) \\cdot q(X_{t}|X_{0})}{q(X_{t-1}|X_{0})}$ \n",
    "\n",
    "Also, if we have a term like $\\sum_{t \\gt 1} log[ q(X_{t-1}|X_{t}, X_{0}) \\cdot \\frac{ q(X_{t}|X_{0})}{q(X_{t-1}|X_{0})}]$\n",
    "\n",
    "We can rearrange to simplify like so;\n",
    "\n",
    "$[\\sum_{t \\gt 1} log q(X_{t-1}|X_{t}, X_{0}) ] +  [\\sum_{t \\gt 1}log \\frac{ q(X_{t}|X_{0})}{q(X_{t-1}|X_{0})}]$ = $[\\sum_{t \\gt 1} log q(X_{t-1}|X_{t}, X_{0}) ] +  [\\sum_{t \\gt 1}log  q(X_{t}|X_{0})] - [\\sum_{t \\gt 1}log {q(X_{t-1}|X_{0})}]$ \n",
    "\n",
    "As you can see from the last two terms, we can do a lot of cancelling and write it as:\n",
    "\n",
    "$=[\\sum_{t \\gt 1} log q(X_{t-1}|X_{t}, X_{0}) ] + log q(X_{T}|X_{0}) - log q(X_1|X_0)$\n",
    "\n",
    "Applying this to our loss function, we end up with;\n",
    "\n",
    "$ = E_{q}[-log \\frac{P_{\\theta}(X_{T})}{q(X_{T}|X_{0})} - \\sum_{t \\gt 1} \\frac{log P_{\\theta}(X_{t-1}|X_{t})}{q(X_{t-1}|X_{t}, X_{0})} - log P_{\\theta}(X_{0}|X_{1})]$\n",
    "\n",
    "Which is our desired result of a sum of KL divergences (plus a small extra term)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826b7078-7ca7-4821-8086-0bfc3d0e7823",
   "metadata": {},
   "source": [
    "## Last steps\n",
    "\n",
    "The final steps in achieving the final loss function are;\n",
    "1. Apply the formula for the KLD between two gaussians.\n",
    "2. Use straight-forward sums of gaussians to compute $ q(X_{t}|X_{0}$ for any t.\n",
    "3. Reparamterise the neural network from predicting the mean (std is fixed here), to predicting the change in the mean i.e the noise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
